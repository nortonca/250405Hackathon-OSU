<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>VAD Debug + Transcription</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.6.1/socket.io.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.22/dist/bundle.min.js"></script>
</head>
<<<<<<< HEAD
<body class="bg-gray-100 min-h-screen">
    <div class="container mx-auto px-4 py-8">
        <header class="mb-8">
            <h1 class="text-3xl font-bold text-center text-blue-600">Voice AI Assistant</h1>
            <p class="text-center text-gray-600 mt-2">Speak to interact with the AI assistant in real-time</p>
        </header>

        <main class="bg-white rounded-lg shadow-md p-6 max-w-lg mx-auto">
            <div id="status" class="mb-4 p-3 bg-gray-100 rounded-lg text-center">
                Ready to detect voice
            </div>

            <div class="flex flex-col space-y-4">
                <button id="start-button" class="bg-blue-500 hover:bg-blue-600 text-white px-4 py-2 rounded transition">
                    Start Listening
                </button>
                <button id="stop-button" class="bg-red-500 hover:bg-red-600 text-white px-4 py-2 rounded transition hidden">
                    Stop Listening
                </button>

                <!-- Live camera section -->
                <div class="mt-4 p-4 border border-gray-200 rounded-lg">
                    <h3 class="text-lg font-medium mb-2">Camera Feed</h3>
                    <p class="text-sm text-gray-500 mb-3">Your camera will automatically capture when you speak</p>

                    <div class="flex flex-col items-center justify-center w-full">
                        <!-- Live video feed -->
                        <div class="relative w-full">
                            <video id="camera-feed" autoplay playsinline class="w-full rounded-lg"></video>
                            <div id="camera-status" class="absolute bottom-2 right-2 bg-gray-800 text-white text-xs px-2 py-1 rounded-full opacity-75">Camera inactive</div>
                        </div>
                        
                        <!-- Camera controls -->
                        <div class="flex mt-2 space-x-2">
                            <button id="start-camera" class="bg-blue-500 hover:bg-blue-600 text-white px-3 py-1 rounded text-sm">
                                Start Camera
                            </button>
                            <button id="stop-camera" class="bg-red-500 hover:bg-red-600 text-white px-3 py-1 rounded text-sm hidden">
                                Stop Camera
                            </button>
                        </div>
                    </div>

                    <!-- Last captured frame -->
                    <div id="captured-frame-container" class="mt-4 hidden">
                        <p class="text-sm font-medium mb-2">Last Captured Frame:</p>
                        <div class="relative">
                            <canvas id="captured-frame" class="max-h-48 rounded-lg mx-auto object-contain border border-gray-300"></canvas>
                        </div>
                    </div>
                </div>
            </div>

            <div id="transcript-container" class="mt-6 h-64 overflow-y-auto border border-gray-200 rounded p-4">
                <div class="text-gray-500 text-center">Transcriptions will appear here</div>
            </div>
        </main>

        <footer class="mt-8 text-center text-gray-500 text-sm">
            <p>¬© 2025 - Voice Transcription App</p>
        </footer>
=======
<body class="bg-gray-100">
  <div class="container mx-auto py-10">
    <h1 class="text-3xl font-bold text-center mb-6">VAD Debug + Transcription</h1>
    <div class="flex justify-center gap-4 mb-4">
      <button id="start" class="bg-blue-600 text-white px-4 py-2 rounded">Start Listening</button>
      <button id="stop" class="bg-red-600 text-white px-4 py-2 rounded">Stop Listening</button>

  <script>
    const log = document.getElementById('log');
    const socket = io();
    let vadInstance = null;


    function debug(msg) {
      const time = new Date().toLocaleTimeString();
      log.textContent += `[${time}] ${msg}\n`;
      log.scrollTop = log.scrollHeight;
      console.log(msg);
    }

    socket.on('connect', () => {
      debug('üîå Socket.IO connected');
    });

    socket.on('transcription_result', (data) => {
      const who = data.type === 'assistant' ? 'ü§ñ AI' : 'üó£Ô∏è You';
      debug(`${who}: ${data.text}`);
    });

    function float32ArrayToWav(audio) {
      const sampleRate = 16000;
      const int16Array = new Int16Array(audio.length);
      for (let i = 0; i < audio.length; i++) {
        const s = Math.max(-1, Math.min(1, audio[i]));
        int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      const buffer = new ArrayBuffer(44 + int16Array.length * 2);
      const view = new DataView(buffer);
      writeString(view, 0, 'RIFF');
      view.setUint32(4, 36 + int16Array.length * 2, true);
      writeString(view, 8, 'WAVE');
      writeString(view, 12, 'fmt ');
      view.setUint32(16, 16, true);
      view.setUint16(20, 1, true);
      view.setUint16(22, 1, true);
      view.setUint32(24, sampleRate, true);
      view.setUint32(28, sampleRate * 2, true);
      view.setUint16(32, 2, true);
      view.setUint16(34, 16, true);
      writeString(view, 36, 'data');
      view.setUint32(40, int16Array.length * 2, true);
      for (let i = 0; i < int16Array.length; i++) {
        view.setInt16(44 + i * 2, int16Array[i], true);
      }
      return new Blob([buffer], { type: 'audio/wav' });
    }

    function writeString(view, offset, string) {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    }

            // Camera handling
            startCameraBtn.addEventListener('click', async function() {
                if (await CameraManager.startCamera()) {
                    startCameraBtn.classList.add('hidden');
                    stopCameraBtn.classList.remove('hidden');
                    updateCameraStatus('Camera active');
                    updateStatus('Camera started successfully');
                } else {
                    updateStatus('Failed to start camera. Please check permissions.', true);
                }
            });
            
            stopCameraBtn.addEventListener('click', function() {
                CameraManager.stopCamera();
                stopCameraBtn.classList.add('hidden');
                startCameraBtn.classList.remove('hidden');
                updateCameraStatus('Camera inactive');
                updateStatus('Camera stopped');
            });

            async function initVAD() {
                try {
                    // Set up VAD with optimized settings for faster response
                    vadInstance = await vad.MicVAD.new({
                        onSpeechStart: () => {
                            updateStatus('Speech detected! Recording...');
                            console.log("Speech start detected");
                            isRecording = true;
                            
                            // Capture camera frame if camera is active
                            if (CameraManager.isActive) {
                                CameraManager.captureFrame();
                                console.log("Frame captured at speech start");
                            }
                        },
                        onSpeechEnd: async (audio) => {
                            console.log("Speech ended, processing in pipeline");
                            isRecording = false;

                            try {
                                // Get current image data if available
                                let imageData = null;
                                const currentImageKey = localStorage.getItem('current-image-key');
                                if (currentImageKey) {
                                    imageData = localStorage.getItem(currentImageKey);
                                }

                                // Check if AudioProcessor exists before using it
                                if (typeof AudioProcessor === 'undefined') {
                                    console.error("AudioProcessor is not defined. Make sure main.js is loaded correctly.");
                                    updateStatus('Error: Audio processing is not available', true);
                                    return;
                                }

                                // Process audio with conversation history
                                const result = await AudioProcessor.processPipeline(audio, imageData, conversationHistory);

                                // Update local conversation history with the new exchange
                                if (result.transcript) {
                                    // Add user message
                                    conversationHistory.push({
                                        "role": "user",
                                        "content": result.transcript
                                    });

                                    // Add assistant response
                                    if (result.response) {
                                        conversationHistory.push({
                                            "role": "assistant",
                                            "content": result.response
                                        });
                                    }

                                    // Keep conversation history to a reasonable size (last 10 messages)
                                    if (conversationHistory.length > 11) { // 1 system + 10 messages
                                        conversationHistory = [
                                            conversationHistory[0], 
                                            ...conversationHistory.slice(conversationHistory.length - 10)
                                        ];
                                    }

                                    console.log("Updated conversation history:", conversationHistory);
                                }
                            } catch (error) {
                                console.error("Error in audio processing:", error);
                                updateStatus('Error: ' + error.message, true);
                            }
                        },
                        onVADMisfire: () => {
                            console.log("VAD misfire (not speech)");
                        }
                    });
                    return true;
                } catch (error) {
                    console.error("Error initializing VAD:", error);
                    updateStatus('Error initializing voice detection', true);
                    return false;
                }
            }
    document.getElementById('start').onclick = async () => {
      debug('Initializing VAD...');
      vadInstance = await vad.MicVAD.new({
        onSpeechStart: () => {
          debug('üü¢ Speech started');
        },
        onSpeechEnd: async (audio) => {
          debug('üî¥ Speech ended');
          if (!audio || !audio.length) {
            debug('‚ö†Ô∏è No audio captured');
            return;
          }

          const wavBlob = float32ArrayToWav(audio);
          const formData = new FormData();
          formData.append('audio', wavBlob, 'speech.wav');

          debug('üì§ Sending audio to /transcribe');

          const xhr = new XMLHttpRequest();
          xhr.open('POST', '/transcribe', true);
          xhr.onload = function () {
            debug(`‚úÖ Response from /transcribe: ${xhr.status}`);
          };
          xhr.onerror = function () {
            debug('‚ùå Error sending audio');
          };
          xhr.send(formData);
        },
        minSpeechFrames: 5,
        preSpeechPadFrames: 10,
        redemptionFrames: 20,
        maxSpeechFrames: 200
      });

      await vadInstance.start();
      debug('üéôÔ∏è Listening...');
    };

    document.getElementById('stop').onclick = () => {
      if (vadInstance) {
        vadInstance.destroy();
        vadInstance = null;
        debug('üõë Stopped VAD');
      }
    };
  </script>

</body>
</html>
