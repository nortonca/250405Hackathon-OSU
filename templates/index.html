<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Transcription</title>
    <!-- Tailwind CSS from CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Socket.IO from CDN -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.6.1/socket.io.js"></script>
    <!-- VAD (Voice Activity Detection) scripts -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.22/dist/bundle.min.js"></script>
</head>
<body class="bg-gray-100 min-h-screen">
    <div class="container mx-auto px-4 py-8">
        <header class="mb-8">
            <h1 class="text-3xl font-bold text-center text-blue-600">Voice AI Assistant</h1>
            <p class="text-center text-gray-600 mt-2">Speak to interact with the AI assistant in real-time</p>
        </header>

        <main class="bg-white rounded-lg shadow-md p-6 max-w-lg mx-auto">
            <div id="status" class="mb-4 p-3 bg-gray-100 rounded-lg text-center">
                Ready to detect voice
            </div>

            <div class="flex flex-col space-y-4">
                <button id="start-button" class="bg-blue-500 hover:bg-blue-600 text-white px-4 py-2 rounded transition">
                    Start Listening
                </button>
                <button id="stop-button" class="bg-red-500 hover:bg-red-600 text-white px-4 py-2 rounded transition hidden">
                    Stop Listening
                </button>

                <!-- Live camera section -->
                <div class="mt-4 p-4 border border-gray-200 rounded-lg">
                    <h3 class="text-lg font-medium mb-2">Camera Feed</h3>
                    <p class="text-sm text-gray-500 mb-3">Your camera will automatically capture when you speak</p>

                    <div class="flex flex-col items-center justify-center w-full">
                        <!-- Live video feed -->
                        <div class="relative w-full">
                            <video id="camera-feed" autoplay playsinline class="w-full rounded-lg"></video>
                            <div id="camera-status" class="absolute bottom-2 right-2 bg-gray-800 text-white text-xs px-2 py-1 rounded-full opacity-75">Camera inactive</div>
                        </div>
                        
                        <!-- Camera controls -->
                        <div class="flex mt-2 space-x-2">
                            <button id="start-camera" class="bg-blue-500 hover:bg-blue-600 text-white px-3 py-1 rounded text-sm">
                                Start Camera
                            </button>
                            <button id="stop-camera" class="bg-red-500 hover:bg-red-600 text-white px-3 py-1 rounded text-sm hidden">
                                Stop Camera
                            </button>
                        </div>
                    </div>

                    <!-- Last captured frame -->
                    <div id="captured-frame-container" class="mt-4 hidden">
                        <p class="text-sm font-medium mb-2">Last Captured Frame:</p>
                        <div class="relative">
                            <canvas id="captured-frame" class="max-h-48 rounded-lg mx-auto object-contain border border-gray-300"></canvas>
                        </div>
                    </div>

                    <!-- Image history section -->
                    <div class="mt-4">
                        <div class="flex items-center justify-between">
                            <h4 class="text-sm font-medium text-gray-700">Image Context History</h4>

                <button id="clear-context" class="bg-gray-500 hover:bg-gray-600 text-white px-4 py-2 rounded transition">
                    Clear Context
                </button>

                            <span id="context-count" class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded-full">0 images</span>
                        </div>
                        <p class="text-xs text-gray-500 mb-2">Previous images used for context</p>
                        <div id="image-history-container" class="flex flex-wrap gap-2 mt-2 p-2 border border-gray-200 rounded-md bg-gray-50 min-h-[60px]">
                            <p class="text-gray-500 text-sm text-center w-full">No image history</p>
                        </div>
                    </div>
                </div>
            </div>

            <div id="transcript-container" class="mt-6 h-64 overflow-y-auto border border-gray-200 rounded p-4">
                <div class="text-gray-500 text-center">Transcriptions will appear here</div>
            </div>
        </main>

        <footer class="mt-8 text-center text-gray-500 text-sm">
            <p>Â© 2025 - Voice Transcription App</p>
        </footer>
    </div>

    <!-- Main JS -->
    <script src="{{ url_for('static', filename='js/main.js') }}"></script>
    
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            // Socket.IO for real-time communication
            const socket = io();
            const startButton = document.getElementById('start-button');
            const stopButton = document.getElementById('stop-button');
            const statusDiv = document.getElementById('status');
            const transcriptContainer = document.getElementById('transcript-container');
            let vadInstance = null;
            let audioChunks = [];
            let isRecording = false;
            
            // Initialize image history display
            updateImageContextInfo();

            // Add conversation history tracking
            let conversationHistory = [
                {"role": "system", "content": "You are a helpful assistant responding to voice transcriptions and image analysis. Keep responses concise and natural."}
            ];

            // Handle connection events
            socket.on('connect', () => {
                console.log('Connected to server');
            });

            socket.on('transcription_result', (data) => {
                if (data.text) {
                    addTranscript(data.text, data.type);
                }
            });

            socket.on('image_uploaded', (data) => {
                updateStatus('Image uploaded and ready for analysis');
                console.log('Image uploaded:', data);
            });

            // Camera variables and elements
            const startCameraBtn = document.getElementById('start-camera');
            const stopCameraBtn = document.getElementById('stop-camera');
            const cameraFeed = document.getElementById('camera-feed');
            const capturedFrame = document.getElementById('captured-frame');
            const cameraStatus = document.getElementById('camera-status');
            
            // Initialize camera manager
            CameraManager.init('camera-feed', 'captured-frame');
            
            function updateStatus(message, isError = false) {
                statusDiv.textContent = message;
                statusDiv.className = `mb-4 p-3 rounded-lg text-center ${isError ? 'bg-red-100 text-red-700' : 'bg-gray-100'}`;
            }
            
            function updateCameraStatus(message) {
                cameraStatus.textContent = message;
            }
            
            function updateImageContextInfo() {
                const contextCount = document.getElementById('context-count');
                if (contextCount) {
                    const count = ImageHistoryManager.getHistory().length;
                    contextCount.textContent = `${count} ${count === 1 ? 'image' : 'images'}`;
                    contextCount.className = `text-xs ${count > 0 ? 'bg-blue-100 text-blue-800' : 'bg-gray-100 text-gray-600'} px-2 py-1 rounded-full`;
                }
                
                // Update the image history display
                ImageHistoryManager.renderInUI('image-history-container');
            }

            function addTranscript(text, type = 'user') {
                // Clear the placeholder if it's the first transcript
                if (transcriptContainer.querySelector('.text-gray-500')) {
                    transcriptContainer.innerHTML = '';
                }

                const transcriptDiv = document.createElement('div');
                
                if (type === 'user') {
                    transcriptDiv.className = 'mb-2 p-2 bg-blue-50 rounded self-end relative';
                    transcriptDiv.innerHTML = '<span class="font-semibold">You:</span> ' + text;
                    
                    // Add image indicator if there's a current image
                    const currentImageKey = localStorage.getItem('current-image-key');
                    if (currentImageKey) {
                        const imageIndicator = document.createElement('div');
                        imageIndicator.className = 'absolute -top-2 -left-2 bg-blue-500 text-white rounded-full p-1 w-5 h-5 flex items-center justify-center text-xs';
                        imageIndicator.innerHTML = '<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-3 h-3"><path stroke-linecap="round" stroke-linejoin="round" d="M6.827 6.175A2.31 2.31 0 015.186 7.23c-.38.054-.757.112-1.134.175C2.999 7.58 2.25 8.507 2.25 9.574V18a2.25 2.25 0 002.25 2.25h15A2.25 2.25 0 0021.75 18V9.574c0-1.067-.75-1.994-1.802-2.169a47.865 47.865 0 00-1.134-.175 2.31 2.31 0 01-1.64-1.055l-.822-1.316a2.192 2.192 0 00-1.736-1.039 48.774 48.774 0 00-5.232 0 2.192 2.192 0 00-1.736 1.039l-.821 1.316z" /><path stroke-linecap="round" stroke-linejoin="round" d="M16.5 12.75a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM18.75 10.5h.008v.008h-.008V10.5z" /></svg>';
                        transcriptDiv.appendChild(imageIndicator);
                    }
                } else {
                    transcriptDiv.className = 'mb-2 p-2 bg-green-50 rounded self-start relative';
                    transcriptDiv.innerHTML = '<span class="font-semibold">AI:</span> ' + text;
                    
                    // Add indicator if we have image history
                    const imageHistory = ImageHistoryManager.getHistory();
                    if (imageHistory.length > 0) {
                        const historyIndicator = document.createElement('div');
                        historyIndicator.className = 'absolute -top-2 -right-2 bg-green-500 text-white rounded-full p-1 flex items-center justify-center text-xs';
                        historyIndicator.style.minWidth = '20px';
                        historyIndicator.style.height = '20px';
                        historyIndicator.textContent = imageHistory.length;
                        historyIndicator.title = `Using ${imageHistory.length} image${imageHistory.length > 1 ? 's' : ''} for context`;
                        transcriptDiv.appendChild(historyIndicator);
                    }
                }
                
                transcriptContainer.appendChild(transcriptDiv);
                transcriptContainer.scrollTop = transcriptContainer.scrollHeight;
            }

            // Camera handling
            startCameraBtn.addEventListener('click', async function() {
                if (await CameraManager.startCamera()) {
                    startCameraBtn.classList.add('hidden');
                    stopCameraBtn.classList.remove('hidden');
                    updateCameraStatus('Camera active');
                    updateStatus('Camera started successfully');
                } else {
                    updateStatus('Failed to start camera. Please check permissions.', true);
                }
            });
            
            stopCameraBtn.addEventListener('click', function() {
                CameraManager.stopCamera();
                stopCameraBtn.classList.add('hidden');
                startCameraBtn.classList.remove('hidden');
                updateCameraStatus('Camera inactive');
                updateStatus('Camera stopped');
            });

            async function initVAD() {
                try {
                    // Set up VAD with optimized settings for faster response
                    vadInstance = await vad.MicVAD.new({
                        onSpeechStart: () => {
                            updateStatus('Speech detected! Recording...');
                            console.log("Speech start detected");
                            isRecording = true;
                            
                            // Capture camera frame if camera is active
                            if (CameraManager.isActive) {
                                CameraManager.captureFrame();
                                console.log("Frame captured at speech start");
                            }
                        },
                        onSpeechEnd: async (audio) => {
                            console.log("Speech ended, processing in pipeline");
                            isRecording = false;

                            try {
                                // Get current image data if available
                                let imageData = null;
                                const currentImageKey = localStorage.getItem('current-image-key');
                                if (currentImageKey) {
                                    imageData = localStorage.getItem(currentImageKey);
                                }
                                
                                // Update the image history UI
                                updateImageContextInfo();

                                // Check if AudioProcessor exists before using it
                                if (typeof AudioProcessor === 'undefined') {
                                    console.error("AudioProcessor is not defined. Make sure main.js is loaded correctly.");
                                    updateStatus('Error: Audio processing is not available', true);
                                    return;
                                }

                                // Process audio with conversation history
                                const result = await AudioProcessor.processPipeline(audio, imageData, conversationHistory);

                                // Update local conversation history with the new exchange
                                if (result.transcript) {
                                    // Get current image data if available
                                    let currentImageData = null;
                                    const currentImageKey = localStorage.getItem('current-image-key');
                                    if (currentImageKey) {
                                        currentImageData = localStorage.getItem(currentImageKey);
                                    }
                                    
                                    // Add user message with image data if available
                                    const userMessage = {
                                        "role": "user",
                                        "content": result.transcript
                                    };
                                    
                                    // If there's an image, store it with the message
                                    if (currentImageData) {
                                        userMessage.has_image = true;
                                        userMessage.image_data = currentImageData;
                                    }
                                    
                                    conversationHistory.push(userMessage);

                                    // Add assistant response
                                    if (result.response) {
                                        conversationHistory.push({
                                            "role": "assistant",
                                            "content": result.response
                                        });
                                    }

                                    // Keep conversation history to a reasonable size (last 10 messages)
                                    if (conversationHistory.length > 11) { // 1 system + 10 messages
                                        conversationHistory = [
                                            conversationHistory[0], 
                                            ...conversationHistory.slice(conversationHistory.length - 10)
                                        ];
                                    }

                                    console.log("Updated conversation history:", conversationHistory);
                                }
                            } catch (error) {
                                console.error("Error in audio processing:", error);
                                updateStatus('Error: ' + error.message, true);
                            }
                        },
                        onVADMisfire: () => {
                            console.log("VAD misfire (not speech)");
                        }
                    });
                    return true;
                } catch (error) {
                    console.error("Error initializing VAD:", error);
                    updateStatus('Error initializing voice detection', true);
                    return false;
                }
            }

            // Function to convert Float32Array to WAV format
            function float32ArrayToWav(audio) {
                // The VAD outputs audio at 16kHz
                const sampleRate = 16000;
                const numChannels = 1;
                const bitsPerSample = 16;

                // Convert Float32Array to Int16Array
                const int16Array = new Int16Array(audio.length);
                for (let i = 0; i < audio.length; i++) {
                    // Convert float to int16
                    const s = Math.max(-1, Math.min(1, audio[i]));
                    int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                }

                // Create the WAV file
                const buffer = new ArrayBuffer(44 + int16Array.length * 2);
                const view = new DataView(buffer);

                // RIFF header
                writeString(view, 0, 'RIFF');
                view.setUint32(4, 36 + int16Array.length * 2, true);
                writeString(view, 8, 'WAVE');

                // fmt chunk
                writeString(view, 12, 'fmt ');
                view.setUint32(16, 16, true);
                view.setUint16(20, 1, true); // PCM format
                view.setUint16(22, numChannels, true);
                view.setUint32(24, sampleRate, true);
                view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true);
                view.setUint16(32, numChannels * bitsPerSample / 8, true);
                view.setUint16(34, bitsPerSample, true);

                // data chunk
                writeString(view, 36, 'data');
                view.setUint32(40, int16Array.length * 2, true);

                // Write the PCM samples
                const offset = 44;
                for (let i = 0; i < int16Array.length; i++) {
                    view.setInt16(offset + i * 2, int16Array[i], true);
                }

                return new Blob([buffer], { type: 'audio/wav' });
            }

            function writeString(view, offset, string) {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            }


            startButton.addEventListener('click', async () => {
                startButton.classList.add('hidden');
                stopButton.classList.remove('hidden');
                updateStatus('Initializing voice detection...');

                if (await initVAD()) {
                    vadInstance.start();
                    updateStatus('Listening for speech...');
                } else {
                    startButton.classList.remove('hidden');
                    stopButton.classList.add('hidden');
                }
            });

            stopButton.addEventListener('click', () => {
                if (vadInstance) {
                    vadInstance.destroy();
                    vadInstance = null;
                }
                stopButton.classList.add('hidden');
                startButton.classList.remove('hidden');
                updateStatus('Voice detection stopped');
                
                // Clear conversation and image context
                clearAllContext();
            });
            
            // Function to clear all context
            function clearAllContext() {
                // Clear conversation history
                conversationHistory = [
                    {"role": "system", "content": "You are a helpful assistant responding to voice transcriptions and image analysis. Keep responses concise and natural."}
                ];
                
                // Clear image history
                ImageHistoryManager.clearAllHistory();
                
                // Clear captured frame
                CameraManager.clearCapturedFrame();
                
                // Update image context count
                updateImageContextInfo();
                
                // Clear transcript container
                transcriptContainer.innerHTML = '<div class="text-gray-500 text-center">Transcriptions will appear here</div>';
                
                // Show status notification
                updateStatus('All conversation and image context cleared');
            }

            // Set up clear context button
            const clearContextBtn = document.getElementById('clear-context');
            clearContextBtn.addEventListener('click', () => {
                clearAllContext();
            });
            
            // Auto-start camera if permissions are already granted
            navigator.mediaDevices.getUserMedia({ video: true })
                .then(stream => {
                    stream.getTracks().forEach(track => track.stop());
                    startCameraBtn.click();
                })
                .catch(err => {
                    console.log("Camera requires manual activation", err);
                });
        });
    </script>
</body>
</html>